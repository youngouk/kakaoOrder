import os
import json
import re
import traceback
import logging
from datetime import datetime
import pathlib
from typing import List, Dict, Any, Optional, Set
import concurrent.futures
from collections import defaultdict

import anthropic

from config import ANTHROPIC_API_KEY
from utils.text_processing import filter_conversation_by_date, split_conversation_into_chunks
from utils.validation import validate_analysis_result, filter_invalid_items, is_valid_item_name
from services.preprocess_chat import ChatPreprocessor

# Initialize Claude client
client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

# ì±„íŒ… ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
chat_preprocessor = ChatPreprocessor()

def analyze_conversation(
    conversation_text: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    shop_name: Optional[str] = None
) -> Dict[str, Any]:
    """
    í´ë¡œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ê³  ì£¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    print(f"Starting analysis: shop_name={shop_name}, start_date={start_date}, end_date={end_date}")
    print(f"ì›ë³¸ ëŒ€í™” ê¸¸ì´: {len(conversation_text)} ë¬¸ì")
    
    # 1. ë‚ ì§œ í•„í„°ë§
    preprocessed_text = conversation_text
    if start_date or end_date:
        filtered_text = filter_conversation_by_date(conversation_text, start_date, end_date)
        print(f"ë‚ ì§œ í•„í„°ë§ í›„ ëŒ€í™” ê¸¸ì´: {len(filtered_text)} ë¬¸ì")
        
        if filtered_text == "ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.":
            print("âš ï¸ ê²½ê³ : ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "error": True,
                "message": "ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤."
            }
            
        preprocessed_text = filtered_text
    
    # 2. ë¶ˆí•„ìš”í•œ ë©”ì‹œì§€ ì œê±°
    try:
        # ì „ì²˜ë¦¬ ì „ì— ì›ë³¸ ëŒ€í™”ì— ëŒ€í•œ í†µê³„ ì¶œë ¥
        stats = chat_preprocessor.get_statistics(preprocessed_text)
        print("ëŒ€í™” í†µê³„:")
        print(f"  - ì „ì²´ ë©”ì‹œì§€: {stats['ì „ì²´ ë©”ì‹œì§€']}ì¤„")
        print(f"  - ì…ì¥ ë©”ì‹œì§€: {stats['ì…ì¥ ë©”ì‹œì§€']}ì¤„")
        print(f"  - í‡´ì¥ ë©”ì‹œì§€: {stats['í‡´ì¥ ë©”ì‹œì§€']}ì¤„")
        print(f"  - ì‚­ì œëœ ë©”ì‹œì§€: {stats['ì‚­ì œëœ ë©”ì‹œì§€']}ì¤„")
        print(f"  - ë´‡ ë©”ì‹œì§€: {stats['ë´‡ ë©”ì‹œì§€']}ì¤„")
        print(f"  - ë¯¸ë””ì–´ ë©”ì‹œì§€: {stats['ë¯¸ë””ì–´ ë©”ì‹œì§€']}ì¤„")
        print(f"  - ë‚ ì§œ êµ¬ë¶„ì„ : {stats['ë‚ ì§œ êµ¬ë¶„ì„ ']}ì¤„")
        
        # ì „ì²˜ë¦¬ ì‹¤í–‰
        preprocessed_text = chat_preprocessor.preprocess_chat(preprocessed_text)
        print(f"ì „ì²˜ë¦¬ í›„ ëŒ€í™” ê¸¸ì´: {len(preprocessed_text)} ë¬¸ì")
        
        # ì „ì²˜ë¦¬ëœ ëŒ€í™” ì €ì¥ (ì„ íƒì )
        _save_preprocessed_text(preprocessed_text, shop_name)
        
    except Exception as e:
        print(f"ëŒ€í™” ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("í•„í„°ë§ëœ ëŒ€í™”ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # 3. íŒë§¤ì ë©”ì‹œì§€ì—ì„œ íŒë§¤ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
    try:
        from services.product_service import get_available_products, extract_product_info
        # available_productsëŠ” get_available_productsì—ì„œ Set[str] í˜•íƒœë¡œ ìƒí’ˆëª…ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        available_products_set = get_available_products(preprocessed_text)
        
        # product_infoëŠ” extract_product_infoì—ì„œ Dict[str, List[Dict[str, str]]] í˜•íƒœë¡œ ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # ì´ ì •ë³´ë¥¼ LLMì´ ì§ì ‘ ì°¸ê³ í•˜ë„ë¡ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ë” ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # í˜¹ì€ ì—¬ê¸°ì„œ ìƒí’ˆëª… ë¦¬ìŠ¤íŠ¸ë§Œ ë½‘ì•„ì„œ ì „ë‹¬í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ìƒí’ˆëª… ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •í•©ë‹ˆë‹¤.
        # ë§Œì•½ product_info ì „ì²´ë¥¼ í™œìš©í•˜ê³  ì‹¶ë‹¤ë©´, analyze_conversation_chunk ë° _create_user_promptì˜ ì¸ì íƒ€ì… ë³€ê²½ í•„ìš”.
        product_info_dict = extract_product_info(preprocessed_text)
        
        # product_info_dictì—ì„œ ì‹¤ì œ ìƒí’ˆëª… ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ available_products_for_llm ë¡œ ì „ë‹¬
        all_product_names_from_info = set()
        if isinstance(product_info_dict, dict) and "products" in product_info_dict and isinstance(product_info_dict["products"], list):
            for product_detail in product_info_dict["products"]:
                if isinstance(product_detail, dict) and "name" in product_detail:
                    all_product_names_from_info.add(product_detail["name"])
        
        # ë§Œì•½ get_available_productsì˜ ê²°ê³¼ì™€ extract_product_infoì˜ ê²°ê³¼ë¥¼ í•©ì¹˜ê±°ë‚˜, 
        # ë‘˜ ì¤‘ ë” ì‹ ë¢°ë„ ë†’ì€ ê²ƒì„ ì„ íƒí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” extract_product_info ê²°ê³¼ë¥¼ ìš°ì„ í•©ë‹ˆë‹¤.
        final_product_list_for_llm = all_product_names_from_info if all_product_names_from_info else available_products_set

        print(f"ì „ì²´ ëŒ€í™”ì—ì„œ ì¶”ì¶œí•œ íŒë§¤ ìƒí’ˆ ì •ë³´ (LLM ì „ë‹¬ìš©): {len(final_product_list_for_llm)}ê°œ ìƒí’ˆ")
        if final_product_list_for_llm:
            print(f"  - ì˜ˆì‹œ ìƒí’ˆ: {', '.join(list(final_product_list_for_llm)[:5])}{'...' if len(final_product_list_for_llm) > 5 else ''}")

    except Exception as e:
        print(f"ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        final_product_list_for_llm = set()
    
    # 4. ëŒ€í™”ê°€ ê¸¸ ê²½ìš° ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
    if len(preprocessed_text) > 60000:
        print(f"ëŒ€í™”ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤({len(preprocessed_text)} ì). ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.")
        chunks = split_conversation_into_chunks(preprocessed_text)
        print(f"{len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìŠ¤ë ˆë“œ í’€ ìƒì„±
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(chunks), 5)) as executor:
            future_to_chunk = {
                executor.submit(analyze_conversation_chunk, chunk, shop_name, final_product_list_for_llm): i 
                for i, chunk in enumerate(chunks)
            }
            
            for future in concurrent.futures.as_completed(future_to_chunk):
                chunk_index = future_to_chunk[future]
                try:
                    result = future.result()
                    print(f"ì²­í¬ {chunk_index} ë¶„ì„ ì™„ë£Œ")
                    results.append(result)
                except Exception as e:
                    print(f"ì²­í¬ {chunk_index} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # ë¶„í•  ê²°ê³¼ ë³‘í•©
        return _merge_chunk_results(results)
    else:
        # ë‹¨ì¼ ì²­í¬ë¡œ ì²˜ë¦¬
        return analyze_conversation_chunk(preprocessed_text, shop_name, final_product_list_for_llm)
    

def _save_preprocessed_text(preprocessed_text: str, shop_name: Optional[str] = None) -> str:
    """
    ì „ì²˜ë¦¬ëœ ëŒ€í™” í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        preprocessed_text (str): ì „ì²˜ë¦¬ëœ ëŒ€í™” ë‚´ìš©
        shop_name (str, optional): ìƒì  ì´ë¦„
        
    Returns:
        str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    # ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    logs_dir = pathlib.Path(__file__).parent.parent / "logs" / "preprocessed_texts"
    logs_dir.mkdir(parents=True, exist_ok=True)
    
    # í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    shop_name_part = f"_{shop_name}" if shop_name else ""
    log_filename = f"preprocessed_text{shop_name_part}_{timestamp}.txt"
    log_file_path = logs_dir / log_filename
    
    # íŒŒì¼ì— ì €ì¥
    try:
        with open(log_file_path, 'w', encoding='utf-8') as f:
            f.write(preprocessed_text)
        print(f"ì „ì²˜ë¦¬ëœ ëŒ€í™”ê°€ {log_file_path} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì „ì²˜ë¦¬ëœ ëŒ€í™” ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return "ì €ì¥ ì‹¤íŒ¨"
    
    return str(log_file_path)

def _save_api_response_to_file(response_content: str, shop_name: Optional[str] = None) -> str:
    """
    Claude API ì‘ë‹µì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        response_content (str): API ì‘ë‹µ ë‚´ìš©
        shop_name (str, optional): ìƒì  ì´ë¦„
        
    Returns:
        str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    # ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    logs_dir = pathlib.Path(__file__).parent.parent / "logs" / "claude_responses"
    logs_dir.mkdir(parents=True, exist_ok=True)
    
    # í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    shop_name_part = f"_{shop_name}" if shop_name else ""
    log_filename = f"claude_response{shop_name_part}_{timestamp}.json"
    log_file_path = logs_dir / log_filename
    
    # ì‘ë‹µ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
    try:
        # JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
        try:
            json_content = json.loads(response_content)
            log_data = {
                "timestamp": datetime.now().isoformat(),
                "shop_name": shop_name,
                "content_type": "json",
                "content": json_content
            }
        except json.JSONDecodeError:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¡œ ì €ì¥
            log_data = {
                "timestamp": datetime.now().isoformat(),
                "shop_name": shop_name,
                "content_type": "text",
                "content": response_content
            }
        
        # íŒŒì¼ì— ì €ì¥
        with open(log_file_path, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥ ì‹œë„
        log_file_path = logs_dir / f"claude_response{shop_name_part}_{timestamp}.txt"
        try:
            with open(log_file_path, 'w', encoding='utf-8') as f:
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Shop Name: {shop_name}\n")
                f.write("Error during JSON serialization\n")
                f.write("-" * 80 + "\n")
                f.write(response_content)
        except Exception as text_write_error:
            print(f"API ì‘ë‹µ ë¡œê¹… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(text_write_error)}")
            return "ë¡œê¹… ì‹¤íŒ¨"
    
    return str(log_file_path)

def analyze_conversation_chunk(conversation_chunk: str, shop_name: Optional[str] = None, product_list_for_llm: Optional[Set[str]] = None) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ëŒ€í™” ì²­í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. (ë©”ì¸ í˜¸ì¶œ: thinking ëª¨ë“œ, JSON ì§ì ‘ ë°˜í™˜ ìš”ì²­)
    """
    try:
        if product_list_for_llm is None:
            # product_list_for_llmì´ analyze_conversationì—ì„œ ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš° (ì˜ˆ: ì§ì ‘ í˜¸ì¶œ ì‹œ)
            # ì—¬ê¸°ì„œëŠ” get_available_products ë˜ëŠ” extract_product_info ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
            # ì¼ê´€ì„±ì„ ìœ„í•´ analyze_conversationê³¼ ìœ ì‚¬í•œ ë¡œì§ì„ ë”°ë¦…ë‹ˆë‹¤.
            from services.product_service import get_available_products, extract_product_info
            temp_product_info_dict = extract_product_info(conversation_chunk)
            temp_all_product_names = set()
            if isinstance(temp_product_info_dict, dict) and "products" in temp_product_info_dict and isinstance(temp_product_info_dict["products"], list):
                for product_detail in temp_product_info_dict["products"]:
                    if isinstance(product_detail, dict) and "name" in product_detail:
                        temp_all_product_names.add(product_detail["name"])
            
            if not temp_all_product_names: # extract_product_info ê²°ê³¼ê°€ ì—†ë‹¤ë©´ get_available_products ì‚¬ìš©
                 temp_all_product_names = get_available_products(conversation_chunk)
            product_list_for_llm = temp_all_product_names
        
        print(f"LLMì— ì „ë‹¬ë  ìƒí’ˆ ëª©ë¡ (analyze_conversation_chunk): {len(product_list_for_llm)}ê°œ")
        
        # ë©”ì¸ í˜¸ì¶œìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (JSON ì§ì ‘ ë°˜í™˜ ìœ ë„)
        system_prompt = _create_system_prompt(shop_name)
        # ë©”ì¸ í˜¸ì¶œìš© ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        user_prompt = _create_user_prompt(conversation_chunk, product_list_for_llm, for_main_call=True)
        
        print(f"Claude API í˜¸ì¶œ ì¤€ë¹„ (ë©”ì¸ ë¶„ì„, ëŒ€í™” ê¸¸ì´: {len(conversation_chunk)} ì)")
        model_name = "claude-3-7-sonnet-20250219"
        print(f"ì‚¬ìš© ëª¨ë¸: {model_name}")
        
        # ëŒ€ì²´ í˜¸ì¶œì—ì„œ ì‚¬ìš©í•  ë„êµ¬ ì •ì˜
        tools_definition_for_fallback = [{
            "name": "extract_order_info",
            "description": "ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”ì—ì„œ ì£¼ë¬¸ ì •ë³´ ë° íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ",
            "input_schema": {
                "type": "object",
                "properties": {
                    "time_based_orders": {
                        "type": "array",
                        "description": "ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ê°œë³„ ì£¼ë¬¸ ë‚´ì—­ì…ë‹ˆë‹¤. ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ëª¨ë“  ì£¼ë¬¸ì„ **í•˜ë‚˜ë„ ë¹ ì§ì—†ì´, ê°€ëŠ¥í•œ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬** ì—¬ê¸°ì— ê¸°ë¡í•´ì•¼ í•©ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ì£¼ë¬¸ì„ ì„ì˜ë¡œ ëˆ„ë½í•˜ê±°ë‚˜ ìš”ì•½í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ëª¨ë“  ì£¼ë¬¸ ê¸°ë¡ì„ ìƒì„¸íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”.",
                        "items": {
                            "type": "object",
                            "properties": {
                                "time": {"type": "string", "description": "ì£¼ë¬¸ ì‹œê°„ (ì˜ˆ: 'ì˜¤ì „ 9:51', 'ì˜¤í›„ 12:02')"},
                                "customer": {"type": "string", "description": "ì£¼ë¬¸ ê³ ê° ì´ë¦„ ë˜ëŠ” ë‹‰ë„¤ì„ (ì˜ˆ: 'ë¦¬ë¦¬', 'ì‚¼ë‚¨ë§¤ë§˜S2 8605')"},
                                "item": {"type": "string", "description": "ì£¼ë¬¸ í’ˆëª© (ì˜ˆ: 'í”„ë¦¬ë¯¸ì—„ ìš°ì‚¼ê²¹', 'í•œìš°ë‚˜ì£¼ê³°íƒ•')"},
                                "quantity": {"type": "integer", "description": "ì£¼ë¬¸ ìˆ˜ëŸ‰ (ì˜ˆ: 1, 2)"},
                                "note": {"type": "string", "description": "ì£¼ë¬¸ ê´€ë ¨ ì°¸ê³  ì‚¬í•­ (ì˜ˆ: 'í˜„ì¥íŒë§¤', 'ì›”ìš”ì¼ ìˆ˜ë ¹', 'ì·¨ì†Œ'). ì´ í•„ë“œëŠ” í•­ìƒ ì¡´ì¬í•´ì•¼ í•˜ë©°, íŠ¹ì´ì‚¬í•­ì´ ì—†ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´ \"\"ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."}
                            },
                            "required": ["time", "customer", "item", "quantity"]
                        }
                    },
                    "item_based_summary": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "item": {"type": "string", "description": "í’ˆëª©ëª…"},
                                "total_quantity": {"type": "integer", "description": "ì´ ì£¼ë¬¸ ìˆ˜ëŸ‰"},
                                "customers": {"type": "string", "description": "í•´ë‹¹ í’ˆëª© ì£¼ë¬¸ì ëª©ë¡ (ì½¤ë§ˆë¡œ êµ¬ë¶„)"}
                            },
                            "required": ["item", "total_quantity", "customers"]
                        }
                    },
                    "customer_based_orders": {
                        "type": "array",
                        "description": "ê³ ê°ë³„ë¡œ ê·¸ë£¹í™”ëœ ì£¼ë¬¸ ë‚´ì—­ì…ë‹ˆë‹¤. ê° ê³ ê°ì´ ì£¼ë¬¸í•œ ëª¨ë“  í’ˆëª©ê³¼ ìˆ˜ëŸ‰ì„ ìƒì„¸í•˜ê²Œ ê¸°ë¡í•´ì•¼ í•©ë‹ˆë‹¤. `time_based_orders`ì—ì„œ ì¶”ì¶œëœ ëª¨ë“  ì£¼ë¬¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ê³ ê° ê¸°ì¤€ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì—¬ **ëˆ„ë½ ì—†ì´ ëª¨ë“  ì£¼ë¬¸ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤**.",
                        "items": {
                            "type": "object",
                            "properties": {
                                "customer": {"type": "string", "description": "ì£¼ë¬¸ì ì´ë¦„ ë˜ëŠ” ID"},
                                "item": {"type": "string", "description": "ì£¼ë¬¸ í’ˆëª©ëª…"},
                                "quantity": {"type": "integer", "description": "ì£¼ë¬¸ ìˆ˜ëŸ‰"},
                                "note": {"type": "string", "description": "ì¶”ê°€ ë©”ëª¨ ë˜ëŠ” ë¹„ê³ . ì´ í•„ë“œëŠ” í•­ìƒ ì¡´ì¬í•´ì•¼ í•˜ë©°, íŠ¹ì´ì‚¬í•­ì´ ì—†ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´ \"\"ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."}
                            },
                            "required": ["customer", "item", "quantity", "note"]
                        }
                    },
                    "order_pattern_analysis": {
                        "type": "object",
                        "description": "ì£¼ë¬¸ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ëŒ€í™” ë‚´ìš© ì „ì²´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤.",
                        "properties": {
                            "peak_hours": {"type": "array", "items": {"type": "string"}, "description": "ì£¼ë¬¸ì´ ê°€ì¥ ë§ì•˜ë˜ ì‹œê°„ëŒ€ (ì˜ˆ: ['ì˜¤í›„ 12:00-13:00', 'ì˜¤í›„ 9:00-10:00'])"},
                            "popular_items": {"type": "array", "items": {"type": "string"}, "description": "ê°€ì¥ ì¸ê¸° ìˆì—ˆë˜ í’ˆëª© (íŒë§¤ëŸ‰ ìˆœ, ìµœëŒ€ 5ê°œ)"},
                            "sold_out_items": {"type": "array", "items": {"type": "string"}, "description": "í’ˆì ˆëœ í’ˆëª© ëª©ë¡"}
                        },
                        "required": ["peak_hours", "popular_items", "sold_out_items"]
                    }
                },
                "required": ["time_based_orders", "customer_based_orders", "order_pattern_analysis"]
            }
        }]

        try:
            print("ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ API í˜¸ì¶œ ì‹œì‘ (ë©”ì¸ ë¶„ì„ - thinking ëª¨ë“œ, JSON ì§ì ‘ ë°˜í™˜)...")
            # client.messages.create ëŒ€ì‹  client.beta.messages.create ì‚¬ìš©
            # betas íŒŒë¼ë¯¸í„° ì¶”ê°€
            stream_response = client.beta.messages.create(
                model=model_name,
                max_tokens=128000, # ì´ì œ 128K ì‚¬ìš© ê°€ëŠ¥
                system=system_prompt,
                temperature=1.0,
                thinking={
                    "type": "enabled",
                    "budget_tokens": 32000  # í•„ìš”ì— ë”°ë¼ ì´ ê°’ë„ ì¡°ì ˆ ê°€ëŠ¥
                },
                stream=True,
                messages=[
                    {"role": "user", "content": user_prompt}
                ],
                betas=["output-128k-2025-02-19"] # í™•ì¥ ì¶œë ¥ ë² íƒ€ ê¸°ëŠ¥ í™œì„±í™”
            )
            
            print("ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ ì¤‘ (ë©”ì¸ ë¶„ì„)...")
            full_text_response = ""
            chunk_counter = 0
            
                for chunk in stream_response:
                    chunk_counter += 1
                    try:
                        if chunk.type == 'content_block_delta' and hasattr(chunk.delta, 'text'):
                        full_text_response += chunk.delta.text
                    # ignore other chunk types for logging
                except Exception:
                    pass

            print(f"ì´ {chunk_counter}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ (ë©”ì¸ ë¶„ì„)")
            logging.info(f"ì´ {chunk_counter}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ (ë©”ì¸ ë¶„ì„)")

            if full_text_response:
                print("ë©”ì¸ ë¶„ì„ ê²°ê³¼ (í…ìŠ¤íŠ¸)ì—ì„œ JSON ì¶”ì¶œ ì‹œë„ ì¤‘...")
                logging.info("ë©”ì¸ ë¶„ì„ ê²°ê³¼ (í…ìŠ¤íŠ¸)ì—ì„œ JSON ì¶”ì¶œ ì‹œë„ ì¤‘...")
                extracted_json_result = _extract_json_from_text(full_text_response)
                
                if extracted_json_result:
                    print("ë©”ì¸ ë¶„ì„: í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ì„±ê³µ.")
                    logging.info("ë©”ì¸ ë¶„ì„: í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ì„±ê³µ.")
                    log_file_path = _save_api_response_to_file(json.dumps(extracted_json_result, ensure_ascii=False), f"{shop_name}_main_direct_json")
                    print(f"API ì‘ë‹µ (ë©”ì¸ ë¶„ì„ JSON)ì´ {log_file_path} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    return _validate_and_process_result(extracted_json_result, conversation_chunk)
                else:
                    print("ë©”ì¸ ë¶„ì„: í…ìŠ¤íŠ¸ì—ì„œ ìœ íš¨í•œ JSONì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    logging.warning("ë©”ì¸ ë¶„ì„: í…ìŠ¤íŠ¸ì—ì„œ ìœ íš¨í•œ JSONì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                print("ë©”ì¸ ë¶„ì„: LLMìœ¼ë¡œë¶€í„° ì–´ë– í•œ í…ìŠ¤íŠ¸ ì‘ë‹µë„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                logging.warning("ë©”ì¸ ë¶„ì„: LLMìœ¼ë¡œë¶€í„° ì–´ë– í•œ í…ìŠ¤íŠ¸ ì‘ë‹µë„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            print("ë©”ì¸ ë¶„ì„ì—ì„œ ìœ íš¨í•œ JSON ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í•¨, ëŒ€ì²´ í˜¸ì¶œ ì‹œë„")
            logging.warning("ë©”ì¸ ë¶„ì„ì—ì„œ ìœ íš¨í•œ JSON ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í•¨, ëŒ€ì²´ í˜¸ì¶œ ì‹œë„")
            return _fallback_process_with_threading(
                conversation_chunk=conversation_chunk,
                shop_name=shop_name,
                original_user_prompt_for_fallback=user_prompt, # ë©”ì¸ ì‹œë„ì—ì„œ ì‚¬ìš©ëœ user_prompt
                tools_for_fallback=tools_definition_for_fallback,
                available_products=product_list_for_llm # ë³€ê²½ëœ ë³€ìˆ˜ëª… ì‚¬ìš©
            )

        except anthropic.APIError as e:
            error_trace = traceback.format_exc()
            error_message = f"Anthropic API ì˜¤ë¥˜ ë°œìƒ (ë©”ì¸ ë¶„ì„): {str(e)}"
            print(error_message); logging.error(error_message)
            print(f"Traceback: {error_trace}"); logging.error(f"Traceback: {error_trace}")
            print("ë©”ì¸ ë¶„ì„ API ì˜¤ë¥˜, ëŒ€ì²´ í˜¸ì¶œ(ìŠ¤ë ˆë”©)ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            logging.warning("ë©”ì¸ ë¶„ì„ API ì˜¤ë¥˜, ëŒ€ì²´ í˜¸ì¶œ(ìŠ¤ë ˆë”©)ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            return _fallback_process_with_threading(
                conversation_chunk=conversation_chunk, shop_name=shop_name,
                original_user_prompt_for_fallback=user_prompt,
                tools_for_fallback=tools_definition_for_fallback,
                available_products=product_list_for_llm # ë³€ê²½ëœ ë³€ìˆ˜ëª… ì‚¬ìš©
            )
        except Exception as e:
            error_trace = traceback.format_exc()
            error_message = f"ë©”ì¸ ë¶„ì„ API í˜¸ì¶œ ì¤‘ ì¼ë°˜ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            print(error_message); logging.error(error_message)
            print(f"Traceback: {error_trace}"); logging.error(f"Traceback: {error_trace}")
            print("ë©”ì¸ ë¶„ì„ ì¼ë°˜ ì˜¤ë¥˜, ëŒ€ì²´ í˜¸ì¶œ(ìŠ¤ë ˆë”©)ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            logging.warning("ë©”ì¸ ë¶„ì„ ì¼ë°˜ ì˜¤ë¥˜, ëŒ€ì²´ í˜¸ì¶œ(ìŠ¤ë ˆë”©)ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            return _fallback_process_with_threading(
                conversation_chunk=conversation_chunk, shop_name=shop_name,
                original_user_prompt_for_fallback=user_prompt,
                tools_for_fallback=tools_definition_for_fallback,
                available_products=product_list_for_llm # ë³€ê²½ëœ ë³€ìˆ˜ëª… ì‚¬ìš©
            )
            
    except Exception as e:
        error_trace = traceback.format_exc()
        error_message = f"ë©”ì¸ ë¶„ì„ ê³¼ì • ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_message); logging.error(error_message)
        print(f"Traceback: {error_trace}"); logging.error(f"Traceback: {error_trace}")
        return {"error": True, "message": error_message, "error_type": "UNEXPECTED_ANALYSIS_ERROR", "traceback": error_trace, "timestamp": datetime.now().isoformat()}

def _create_system_prompt(shop_name: Optional[str] = None) -> str:
    """
    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (ë©”ì¸ ë¶„ì„ìš© - JSON ì§ì ‘ ë°˜í™˜ ìš”ì²­)
    LLMì—ê²ŒëŠ” ì‹œê°„ìˆœ ì£¼ë¬¸(time_based_orders), ê³ ê°ë³„ ì£¼ë¬¸(customer_based_orders), 
    ê·¸ë¦¬ê³  ì£¼ë¬¸ íŒ¨í„´ ë¶„ì„(order_pattern_analysis)ë§Œ ìš”ì²­í•©ë‹ˆë‹¤.
    
    Args:
        shop_name (str, optional): ìƒì  ì´ë¦„
        
    Returns:
        str: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    """
    prompt = """
ë‹¹ì‹ ì€ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”ì—ì„œ ì£¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìœ ì €ê°€ ì œê³µí•œ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ **ì‹œê°„ìˆœ ì£¼ë¬¸ ë‚´ì—­**, **ê³ ê°ë³„ ì£¼ë¬¸ ë‚´ì—­**, ê·¸ë¦¬ê³  **ì£¼ë¬¸ íŒ¨í„´**ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ ë‹¤ìŒ JSON êµ¬ì¡°ì— ë§ì¶° ì‘ë‹µì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
ì‘ë‹µì€ ë°˜ë“œì‹œ JSON ê°ì²´ë§Œìœ¼ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì— ì œê³µëœ **'ì¶”ì¶œëœ ìƒí’ˆ ëª©ë¡'ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬, í•´ë‹¹ ëª©ë¡ì— ìˆëŠ” ìƒí’ˆëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ì£¼ë¬¸ í’ˆëª©ì„ ì •í™•í•˜ê²Œ ì‹ë³„**í•´ì•¼ í•©ë‹ˆë‹¤. 
ì˜ˆë¥¼ ë“¤ì–´, ëŒ€í™”ì— 'ë‚˜ì£¼ê³°íƒ•'ì´ë¼ê³  ì–¸ê¸‰ë˜ì—ˆê³  ìƒí’ˆ ëª©ë¡ì— 'í•œìš°ë‚˜ì£¼ê³°íƒ•'ì´ ìˆë‹¤ë©´, ì£¼ë¬¸ í’ˆëª©ì€ 'í•œìš°ë‚˜ì£¼ê³°íƒ•'ìœ¼ë¡œ ê¸°ë¡í•´ì•¼ í•©ë‹ˆë‹¤.

ì‘ë‹µì€ ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¥¼ ì •í™•íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:
{
    "time_based_orders": [
        {
            "time": "ì£¼ë¬¸ ì‹œê°„ (ì˜ˆ: 'ì˜¤ì „ 9:51', 'ì˜¤í›„ 12:02', 'ì˜¤í›„ 1:33')",
            "customer": "ì£¼ë¬¸ì ì´ë¦„ ë˜ëŠ” ID (ì˜ˆ: 'ë¦¬ë¦¬', 'ì‚¼ë‚¨ë§¤ë§˜S2 8605', 'í¬ë¦¼ 2821', 'ğŸ‘ 0209')",
            "item": "ì£¼ë¬¸ í’ˆëª©ëª… (ì˜ˆ: 'í”„ë¦¬ë¯¸ì—„ ìš°ì‚¼ê²¹', 'í•œìš°ë‚˜ì£¼ê³°íƒ•', 'í•œìš°ì†¡í™”ë²„ì„¯ í•´ì¥êµ­', 'ê´‘ì–‘í•œëˆë¶ˆê³ ê¸°'). ë°˜ë“œì‹œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì˜ 'ì¶”ì¶œëœ ìƒí’ˆ ëª©ë¡'ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•œ ìƒí’ˆëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”.",
            "quantity": "ì£¼ë¬¸ ìˆ˜ëŸ‰ (ìˆ«ì, ì˜ˆ: 1, 2, 3)",
            "note": "ì¶”ê°€ ë©”ëª¨ ë˜ëŠ” ë¹„ê³  (ì˜ˆ: 'ì›”ìš”ì¼ ìˆ˜ë ¹', 'í˜„ì¥íŒë§¤', 'ì·¨ì†Œ', '')" // ë¹„ê³ ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
        }
    ], // ì¤‘ìš”: ëŒ€í™”ì—ì„œ ì‹ë³„ëœ ëª¨ë“  ê°œë³„ ì£¼ë¬¸ ê±´ì„ ì‹œê°„ ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ë„ ë¹ ì§ì—†ì´ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ì·¨ì†Œ/ë³€ê²½ ì‚¬í•­ì„ ì •í™•íˆ ë°˜ì˜í•˜ì„¸ìš”.
    "customer_based_orders": [
        {
            "customer": "ì£¼ë¬¸ì ì´ë¦„ ë˜ëŠ” ID (ì˜ˆ: 'ì‚¼ë‚¨ë§¤ë§˜S2 8605', 'í¬ë¦¼ 2821', 'ì§ì˜ 3820')",
            "item": "ì£¼ë¬¸ í’ˆëª©ëª… (ì˜ˆ: 'ë‚˜ì£¼ê³°íƒ•', 'í•´ì¥êµ­', 'ì‚¬ê³¨ê³°íƒ•'). ë°˜ë“œì‹œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì˜ 'ì¶”ì¶œëœ ìƒí’ˆ ëª©ë¡'ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•œ ìƒí’ˆëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”.",
            "quantity": "ì£¼ë¬¸ ìˆ˜ëŸ‰ (ìˆ«ì, ì˜ˆ: 1, 2, 3)",
            "note": "ì¶”ê°€ ë©”ëª¨ ë˜ëŠ” ë¹„ê³  (ì˜ˆ: 'ì›”ìš”ì¼ ìˆ˜ë ¹', 'ê¸ˆìš”ì¼ ìˆ˜ë ¹', 'í™”ìš”ì¼ ìˆ˜ë ¹')"
        }
    ], // ì¤‘ìš”: ê° ê³ ê°ì´ ì£¼ë¬¸í•œ ëª¨ë“  ë‚´ì—­ì„ ìƒì„¸íˆ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    "order_pattern_analysis": {
        "peak_hours": ["ì£¼ë¬¸ì´ ë§ì€ ì‹œê°„ëŒ€ (ì˜ˆ: 'ì˜¤í›„ 12:00-13:00', 'ì˜¤í›„ 9:00-10:00', 'ì˜¤í›„ 7:00-8:00')"],
        "popular_items": ["ì¸ê¸° í’ˆëª©ëª… (ì˜ˆ: 'ë‚˜ì£¼ê³°íƒ•', 'í•´ì¥êµ­', 'ì‚¬ê³¨ê³°íƒ•', 'ê´‘ì–‘í•œëˆë¶ˆê³ ê¸°', 'ì´ˆì½”ìƒí¬ë¦¼ì¼€ì´í¬')"],
        "sold_out_items": ["í’ˆì ˆëœ í’ˆëª©ëª… (ì˜ˆ: 'ì˜¤ì´ì†Œë°•ì´', 'ë¡¤ì¼€ìŒ', 'ë‹¹ê·¼', 'ê³ êµ¬ë§ˆì¹©')"]
    }
}

ì£¼ë¬¸ ì •ë³´ëŠ” ë³´í†µ ë‹¤ìŒ íŒ¨í„´ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤:
1. "ë‹‰ë„¤ì„ / í’ˆëª© ìˆ˜ëŸ‰, í’ˆëª© ìˆ˜ëŸ‰" (ì˜ˆ: "ë¦¬ë¦¬ / ìš°ì‚¼ê²¹ 2, ë¡¤ì¼€ì´í¬ 1")
2. "ë‹‰ë„¤ì„ ì „í™”ë²ˆí˜¸ / í’ˆëª© ìˆ˜ëŸ‰ í’ˆëª© ìˆ˜ëŸ‰" (ì˜ˆ: "ì‚¼ë‚¨ë§¤ë§˜S2 8605 / ë‚˜ì£¼ê³°íƒ•2ê°œ ì‚¬ê³¨ê³°íƒ•1ê°œ")
3. "ì „í™”ë²ˆí˜¸ / í’ˆëª© ìˆ˜ëŸ‰ í’ˆëª© ìˆ˜ëŸ‰" (ì˜ˆ: "3563/ í•´ì¥êµ­1 ë‚˜ì£¼ê³°íƒ•1 ì‚¬ê³¨ê³°íƒ•1")
4. "ë‹‰ë„¤ì„ / í’ˆëª© ìˆ˜ëŸ‰" (ì˜ˆ: "íˆ¬ìœ¤ : ë¡¤ì¼€ìµ1")
5. "ì£¼ë¬¸ ì·¨ì†Œ ìš”ì²­ ì‹œ í•´ë‹¹ ì£¼ë¬¸ì„ time_based_orders ë° customer_based_orders ì—ì„œ ì œì™¸í•˜ê±°ë‚˜ noteì— 'ì·¨ì†Œ' ê¸°ë¡"
6. "ì£¼ë¬¸ ë³€ê²½ ìš”ì²­ ì‹œ ìµœì‹  ì£¼ë¬¸ ë‚´ìš©ìœ¼ë¡œ time_based_orders ë° customer_based_orders ì— ë°˜ì˜"
7. ìœ„ íŒ¨í„´ì„ ë²—ì–´ë‚˜ë”ë¼ë„ ì£¼ë¬¸ìœ¼ë¡œ ì¸ì‹í•  ê·¼ê±°ê°€ (ìƒí’ˆëª…, ì£¼ë¬¸ì, ìˆ˜ëŸ‰ì •ë³´) ìˆìœ¼ë©´ ì£¼ë¬¸ìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤. 'í˜„ì¥íŒë§¤' ì–¸ê¸‰ë„ ì£¼ë¬¸ìœ¼ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì£¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ë•Œ ë‹¤ìŒ ì‚¬í•­ì— **ë°˜ë“œì‹œ ì£¼ì˜í•˜ê³  ì² ì €íˆ ì§€ì¼œì£¼ì„¸ìš”**:
- **ì™„ì „ì„±**: ëŒ€í™” ë‚´ì˜ ëª¨ë“  ì£¼ë¬¸ì„ **í•˜ë‚˜ë„ ë¹ ì§ì—†ì´** ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ì£¼ë¬¸ ëˆ„ë½ì€ ì ˆëŒ€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- **ìƒí’ˆëª… ì •í™•ì„±**: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì— ì œê³µëœ 'ì¶”ì¶œëœ ìƒí’ˆ ëª©ë¡'ì„ ê¸°ì¤€ìœ¼ë¡œ ì£¼ë¬¸ í’ˆëª©ëª…ì„ ì •í™•íˆ ê¸°ì¬í•´ì•¼ í•©ë‹ˆë‹¤.
- **ìƒì„¸í•¨**: `item_based_summary`ì˜ `customers` í•„ë“œì™€ ì£¼ë¬¸ì ëª©ë¡ì€ **ëª¨ë“  ì£¼ë¬¸ì ì´ë¦„ì„ ìƒëµ ì—†ì´ ì „ë¶€ ë‚˜ì—´**í•´ì•¼ í•©ë‹ˆë‹¤. 'ì™¸ Oëª…' ë˜ëŠ” 'ë“±'ê³¼ ê°™ì€ ìš”ì•½ í‘œí˜„ì€ **ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€**ì…ë‹ˆë‹¤.
- **íŒë§¤ì ë©”ì‹œì§€ ì œì™¸**: 'ìš°êµ­ìƒ ì‹ ê²€ë‹¨', 'êµ­ë¯¼ìƒíšŒ ë¨¸ìŠ´1' ë“± íŒë§¤ì ê³„ì •ì´ ì‘ì„±í•œ ìƒí’ˆ ì†Œê°œ, ê³µì§€, ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ë“±ì€ ê³ ê° ì£¼ë¬¸ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜¤ì§ ê³ ê°ì´ ì£¼ë¬¸ ì˜ì‚¬ë¥¼ ë°íŒ ë©”ì‹œì§€ë§Œ `time_based_orders`ì— í¬í•¨í•©ë‹ˆë‹¤.
- ë™ì¼ì¸, ë™ì¼ í’ˆëª© ì¤‘ë³µ ì£¼ë¬¸ ì‹œ ìµœì‹ ìœ¼ë¡œ ë°˜ì˜
- í’ˆëª©ëª…, ì£¼ë¬¸ìëª…, ìˆ˜ëŸ‰ ì •í™•íˆ ì¶”ì¶œ
- ëˆ„ë½ ì—†ì´ ì „ì²´ ë°ì´í„° ì œê³µ, ì•½ì‹ ê¸°ì¬ ê¸ˆì§€
- "ë§ˆê°" ìƒí’ˆì€ í’ˆì ˆë¡œ ì²˜ë¦¬
- ì£¼ë¬¸ íŒ¨í„´ ë¶„ì„ (í”¼í¬ ì‹œê°„, ì¸ê¸° ìƒí’ˆ, í’ˆì ˆ ìƒí’ˆ) í¬í•¨

ìµœëŒ€í•œ ë§ì€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ë˜, ì–´ë–¤ ê²½ìš°ì—ë„ ì§€ì •ëœ JSON ìŠ¤í‚¤ë§ˆì™€ ìœ„ì˜ ìƒì„¸ ì§€ì‹œì‚¬í•­ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
"""
    if shop_name:
        prompt += f"\në¶„ì„ ì¤‘ì¸ ëŒ€í™”ëŠ” '{shop_name}' ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤."
    return prompt

def _create_user_prompt(preprocessed_text: str, product_list_for_llm: Optional[Set[str]] = None, for_main_call: bool = True) -> str:
    """
    ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        preprocessed_text (str): ì „ì²˜ë¦¬ëœ ëŒ€í™” ë‚´ìš©
        product_list_for_llm (Set[str], optional): LLMì´ ì°¸ê³ í•  ìƒí’ˆëª… ëª©ë¡
        for_main_call (bool): Trueì´ë©´ ë©”ì¸ í˜¸ì¶œìš© (JSON ì§ì ‘ ìš”ì²­, ë„êµ¬ ì‚¬ìš© ì–¸ê¸‰ ì—†ìŒ), 
                              Falseì´ë©´ ì´ì „ì²˜ëŸ¼ ë„êµ¬ ì‚¬ìš© ì–¸ê¸‰ ê°€ëŠ¥ (í˜„ì¬ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        
    Returns:
        str: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
    """
    product_list_text = ""
    if product_list_for_llm and len(product_list_for_llm) > 0:
        product_list = sorted(list(product_list_for_llm))
        product_list_text = "\n\nì¶”ì¶œëœ ìƒí’ˆ ëª©ë¡ (ì´ ëª©ë¡ì„ ê¸°ì¤€ìœ¼ë¡œ ì£¼ë¬¸ í’ˆëª©ëª…ì„ ì •í™•íˆ ì‹ë³„í•´ì£¼ì„¸ìš”):\n" + "\n".join([f"- {product}" for product in product_list])

    product_guide = ""
    if product_list_text:
        product_guide = (
            "ìœ„ 'ì¶”ì¶œëœ ìƒí’ˆ ëª©ë¡'ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ì£¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ì£¼ë¬¸ í’ˆëª©ëª…ì€ ìœ„ ëª©ë¡ì— ìˆëŠ” ì •í™•í•œ ìƒí’ˆëª…ìœ¼ë¡œ ê¸°ë¡í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "ì˜ˆë¥¼ ë“¤ì–´, ëŒ€í™”ì—ì„œ 'ê¹€ì¹˜'ë¼ê³  ì–¸ê¸‰ë˜ì—ˆê³  ìƒí’ˆ ëª©ë¡ì— 'ë°°ì¶”ê¹€ì¹˜'ê°€ ìˆë‹¤ë©´, ì£¼ë¬¸ í’ˆëª©ì€ 'ë°°ì¶”ê¹€ì¹˜'ë¡œ ê¸°ë¡í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        )
    
    tool_instruction = ""
    if not for_main_call:
        # ì´ ë¶€ë¶„ì€ í˜„ì¬ ë¡œì§ì—ì„œëŠ” ëŒ€ì²´ í˜¸ì¶œ ì‹œì—ë„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ë„êµ¬ ì‚¬ìš©ì„ ê°•ì œí•˜ë¯€ë¡œ,
        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì—ì„œëŠ” ëª…ì‹œì ì¸ ë„êµ¬ ì§€ì‹œê°€ í•„ìˆ˜ëŠ” ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # í•„ìš”ì— ë”°ë¼ `_fallback_process_with_threading`ì—ì„œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“¤ ë•Œ ì¡°ì ˆ ê°€ëŠ¥.
        tool_instruction = "ë°˜ë“œì‹œ extract_order_info ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µí•´ì£¼ì„¸ìš”. ì¼ë°˜ í…ìŠ¤íŠ¸ë‚˜ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì‘ë‹µí•˜ì§€ ë§ˆì„¸ìš”."


    return f"""
ì•„ë˜ ì „ì²˜ë¦¬ëœ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì£¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ëŒ€í™”ì—ì„œ ëˆ„ê°€, ë¬´ì—‡ì„, ì–¼ë§ˆë‚˜ ì£¼ë¬¸í–ˆëŠ”ì§€ ì •í™•í•˜ê²Œ íŒŒì•…í•´ ì£¼ì„¸ìš”.
{product_list_text}

{product_guide}
{tool_instruction}

===== ì „ì²˜ë¦¬ëœ ëŒ€í™” ë‚´ìš© =====
{preprocessed_text}
===================
"""

def _extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text (str): ì‘ë‹µ í…ìŠ¤íŠ¸
        
    Returns:
        Optional[Dict[str, Any]]: ì¶”ì¶œëœ JSON ê°ì²´ ë˜ëŠ” None
    """
    logging.info("í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ì‹œë„ ì¤‘...")
    
    # 1. ë„êµ¬ ì‚¬ìš© íŒ¨í„´ ê²€ìƒ‰
    tool_pattern = r'<tool_use name="extract_order_info">([\s\S]*?)</tool_use>'
    tool_matches = re.findall(tool_pattern, text, re.DOTALL)
    
    for match in tool_matches:
        try:
            json_content = match.strip()
            # ì¤„ë°”ê¿ˆ, íƒ­ ë“± ê³µë°± ë¬¸ì ì •ë¦¬
            json_content = re.sub(r'\s+', ' ', json_content)
            logging.info(f"ë„êµ¬ ì‚¬ìš© íŒ¨í„´ì—ì„œ JSON ë°œê²¬ (ê¸¸ì´: {len(json_content)})")
            return json.loads(json_content)
        except json.JSONDecodeError:
            try:
                # ìˆ˜ì • ì‹œë„
                fixed_json = _fix_json_string(match)
                return json.loads(fixed_json)
            except json.JSONDecodeError:
                logging.warning("ë„êµ¬ ì‚¬ìš© íŒ¨í„´ì—ì„œ ë°œê²¬ëœ JSON íŒŒì‹± ì‹¤íŒ¨")
                continue
    
    # 2. ì¤‘ê´„í˜¸ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¶€ë¶„ ì°¾ê¸° (ê· í˜• ë§ì¶˜ ì¤‘ê´„í˜¸ ê²€ìƒ‰)
    logging.info("ì¤‘ê´„í˜¸ íŒ¨í„´ìœ¼ë¡œ JSON ê²€ìƒ‰ ì¤‘...")
    start_idx = text.find('{')
    if start_idx != -1:
        open_count = 0
        close_count = 0
        json_text = ""
        
        for i in range(start_idx, len(text)):
            if text[i] == '{':
                open_count += 1
            elif text[i] == '}':
                close_count += 1
            
            json_text += text[i]
            
            if open_count > 0 and open_count == close_count:
                # ê· í˜•ì´ ë§ëŠ” JSON ì°¾ìŒ
                try:
                    logging.info(f"ê· í˜• ì¡íŒ ì¤‘ê´„í˜¸ íŒ¨í„´ ë°œê²¬ (ê¸¸ì´: {len(json_text)})")
                    return json.loads(json_text)
                except json.JSONDecodeError:
                    # ìˆ˜ì • ì‹œë„
                    try:
                        fixed_json = _fix_json_string(json_text)
                        logging.info("JSON ìˆ˜ì • í›„ íŒŒì‹± ì‹œë„")
                        return json.loads(fixed_json)
                    except json.JSONDecodeError:
                        logging.warning("ê· í˜• ì¡íŒ ì¤‘ê´„í˜¸ì—ì„œ ë°œê²¬ëœ JSON íŒŒì‹± ì‹¤íŒ¨")
                
                # ë‹¤ìŒ JSON ê°ì²´ ì°¾ê¸° ì‹œë„
                next_start = text.find('{', i + 1)
                if next_start == -1:
                    break
                
                i = next_start - 1
                open_count = 0
                close_count = 0
                json_text = ""
    
    # 3. JSON ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì°¾ê¸°
    logging.info("ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ê²€ìƒ‰ ì¤‘...")
    json_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
    matches = re.findall(json_pattern, text, re.DOTALL)
    
    for match in matches:
        try:
            logging.info(f"ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì—ì„œ JSON ë°œê²¬ (ê¸¸ì´: {len(match)})")
            return json.loads(match)
        except json.JSONDecodeError:
            # ìˆ˜ì • ì‹œë„
            try:
                fixed_json = _fix_json_string(match)
                logging.info("ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ JSON ìˆ˜ì • í›„ íŒŒì‹± ì‹œë„")
                return json.loads(fixed_json)
            except json.JSONDecodeError:
                logging.warning("ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì—ì„œ ë°œê²¬ëœ JSON íŒŒì‹± ì‹¤íŒ¨")
                continue
    
    # 4. time_based_orders íŒ¨í„´ ê²€ìƒ‰ (ë¶€ë¶„ JSON êµ¬ì„±)
    logging.info("í‚¤ì›Œë“œ íŒ¨í„´ìœ¼ë¡œ JSON êµ¬ì„± ì‹œë„...")
    time_based_pattern = r'"time_based_orders"\s*:\s*\[([\s\S]*?)\]'
    item_based_pattern = r'"item_based_summary"\s*:\s*\[([\s\S]*?)\]'
    customer_based_pattern = r'"customer_based_orders"\s*:\s*\[([\s\S]*?)\]'
    
    # íŒ¨í„´ì´ ë°œê²¬ë˜ë©´ í…œí”Œë¦¿ìœ¼ë¡œ JSON êµ¬ì„± ì‹œë„
    if re.search(time_based_pattern, text) or re.search(item_based_pattern, text) or re.search(customer_based_pattern, text):
        try:
            # ê³¨ê²© êµ¬ì„±
            template = '''
            {
                "time_based_orders": [],
                "item_based_summary": [],
                "customer_based_orders": [],
                "table_summary": {
                    "headers": ["í’ˆëª©", "ì´ìˆ˜ëŸ‰", "ì£¼ë¬¸ì"],
                    "rows": []
                },
                "order_pattern_analysis": {
                    "peak_hours": [],
                    "popular_items": [],
                    "sold_out_items": []
                }
            }
            '''
            result = json.loads(template)
            
            # ê° ì„¹ì…˜ ì±„ìš°ê¸°
            time_match = re.search(time_based_pattern, text)
            if time_match:
                time_content = f"[{time_match.group(1)}]"
                try:
                    time_data = json.loads(time_content)
                    result["time_based_orders"] = time_data
                    logging.info("time_based_orders ì„¹ì…˜ ì¶”ì¶œ ì„±ê³µ")
                except:
                    logging.warning("time_based_orders ì„¹ì…˜ íŒŒì‹± ì‹¤íŒ¨")
            
            item_match = re.search(item_based_pattern, text)
            if item_match:
                item_content = f"[{item_match.group(1)}]"
                try:
                    item_data = json.loads(item_content)
                    result["item_based_summary"] = item_data
                    logging.info("item_based_summary ì„¹ì…˜ ì¶”ì¶œ ì„±ê³µ")
                except:
                    logging.warning("item_based_summary ì„¹ì…˜ íŒŒì‹± ì‹¤íŒ¨")
                    
            customer_match = re.search(customer_based_pattern, text)
            if customer_match:
                customer_content = f"[{customer_match.group(1)}]"
                try:
                    customer_data = json.loads(customer_content)
                    result["customer_based_orders"] = customer_data
                    logging.info("customer_based_orders ì„¹ì…˜ ì¶”ì¶œ ì„±ê³µ")
                except:
                    logging.warning("customer_based_orders ì„¹ì…˜ íŒŒì‹± ì‹¤íŒ¨")
            
            # ì„¹ì…˜ ì¤‘ í•˜ë‚˜ë¼ë„ íŒŒì‹±í–ˆìœ¼ë©´ ê²°ê³¼ ë°˜í™˜
            if (result["time_based_orders"] or result["item_based_summary"] or result["customer_based_orders"]):
                logging.info("ë¶€ë¶„ JSON êµ¬ì„± ì„±ê³µ")
                return result
                
        except Exception as e:
            logging.error(f"ë¶€ë¶„ JSON êµ¬ì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    logging.warning("í…ìŠ¤íŠ¸ì—ì„œ ìœ íš¨í•œ JSONì„ ì°¾ì§€ ëª»í•¨")
    return None

def _fix_json_string(json_str: str) -> str:
    """
    ì†ìƒëœ JSON ë¬¸ìì—´ì„ ìˆ˜ì •í•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤.
    
    Args:
        json_str (str): ì†ìƒëœ JSON ë¬¸ìì—´
        
    Returns:
        str: ìˆ˜ì •ëœ JSON ë¬¸ìì—´
    """
    # JSON ë¬¸ìì—´ ì–‘ìª½ ëì˜ ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    json_str = json_str.strip()
    
    # ì‹œì‘ê³¼ ëì´ ì¤‘ê´„í˜¸ê°€ ì•„ë‹ˆë©´ ìˆ˜ì •
    if not json_str.startswith('{'):
        first_brace = json_str.find('{')
        if first_brace != -1:
            json_str = json_str[first_brace:]
    
    if not json_str.endswith('}'):
        last_brace = json_str.rfind('}')
        if last_brace != -1:
            json_str = json_str[:last_brace+1]
    
    # ì¤‘ë³µëœ ì¤‘ê´„í˜¸ ì²˜ë¦¬
    if json_str.count('{') > json_str.count('}'):
        json_str = json_str + '}' * (json_str.count('{') - json_str.count('}'))
    elif json_str.count('{') < json_str.count('}'):
        json_str = '{' * (json_str.count('}') - json_str.count('{')) + json_str
    
    # í°ë”°ì˜´í‘œ ì§ ë§ì¶”ê¸°
    if json_str.count('"') % 2 != 0:
        # ì§ì´ ë§ì§€ ì•ŠëŠ” ë”°ì˜´í‘œ ì²˜ë¦¬
        positions = []
        in_string = False
        for i, char in enumerate(json_str):
            if char == '"' and (i == 0 or json_str[i-1] != '\\'):  # ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ ë”°ì˜´í‘œ
                in_string = not in_string
                positions.append(i)
        
        if positions and len(positions) % 2 != 0:
            # ë§ˆì§€ë§‰ ë¹„ì •ìƒ ë”°ì˜´í‘œ ì œê±°
            json_str = json_str[:positions[-1]] + json_str[positions[-1]+1:]
    
    # ì½¤ë§ˆ ì˜¤ë¥˜ ìˆ˜ì •
    json_str = json_str.replace(',}', '}').replace(',]', ']')
    
    # ëˆ„ë½ëœ ê°’ ìˆ˜ì •
    json_str = json_str.replace(':"",', ':"",').replace(':""}', ':""}')
    
    # ë¬¸ìì—´ ë‚´ì˜ ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ ê°œí–‰ë¬¸ì ìˆ˜ì •
    in_string = False
    fixed_str = []
    for i, char in enumerate(json_str):
        if char == '"' and (i == 0 or json_str[i-1] != '\\'):
            in_string = not in_string
        
        if in_string and char in ['\n', '\r']:
            fixed_str.append('\\n')
        else:
            fixed_str.append(char)
    
    return ''.join(fixed_str)

def _split_input_text(text: str, max_length: int = 20000) -> List[str]:
    """
    ê¸´ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ê¸¸ì´ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        text (str): ë¶„í• í•  í…ìŠ¤íŠ¸
        max_length (int, optional): ìµœëŒ€ ê¸¸ì´. ê¸°ë³¸ê°’ì€ 20000.
        
    Returns:
        List[str]: ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    if len(text) <= max_length:
        return [text]
    
    # ë¬¸ì¥ ë˜ëŠ” ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
    segments = re.split(r'(?<=\.)\s+|\n\n+', text)
    
    chunks = []
    current_chunk = ""
    
    for segment in segments:
        if len(current_chunk) + len(segment) <= max_length:
            current_chunk += segment + ("\n\n" if segment.endswith('.') else " ")
        else:
            chunks.append(current_chunk.strip())
            current_chunk = segment + ("\n\n" if segment.endswith('.') else " ")
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    logging.info(f"ì…ë ¥ í…ìŠ¤íŠ¸ ({len(text)}ì)ë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• í•˜ì˜€ìŠµë‹ˆë‹¤.")
    print(f"ì…ë ¥ í…ìŠ¤íŠ¸ ({len(text)}ì)ë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• í•˜ì˜€ìŠµë‹ˆë‹¤.")
    return chunks

def _fallback_process_with_threading(
    conversation_chunk: str, 
    shop_name: Optional[str],
    original_user_prompt_for_fallback: str, 
    tools_for_fallback: List[Dict],
    available_products: Optional[Set[str]] = None
) -> Dict[str, Any]:
    """
    ë©”ì¸ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ, í•„ìš”í•œ ê²½ìš° ì…ë ¥ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ë¡œ API í˜¸ì¶œì„ ì‹œë„í•©ë‹ˆë‹¤.
    (ëŒ€ì²´ í˜¸ì¶œ: tool ì‚¬ìš© ê°•ì œ, thinking ë¯¸ì‚¬ìš©)
    """
    print("ëŒ€ì²´ ì²˜ë¦¬ ì‹œì‘ (ë³‘ë ¬ ê°€ëŠ¥)...")
    logging.info("ëŒ€ì²´ ì²˜ë¦¬ ì‹œì‘ (ë³‘ë ¬ ê°€ëŠ¥)...")
    
    fallback_system_prompt = f"""
ë‹¹ì‹ ì€ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”ì—ì„œ ì£¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì œê³µí•œ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì£¼ë¬¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³ , ë°˜ë“œì‹œ 'extract_order_info' ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
ì ˆëŒ€ë¡œ ì¼ë°˜ í…ìŠ¤íŠ¸ë‚˜ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì‘ë‹µí•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì˜¤ì§ 'extract_order_info' ë„êµ¬ë§Œì„ ì‚¬ìš©í•œ JSON ì‘ë‹µë§Œ í—ˆìš©ë©ë‹ˆë‹¤.

[ë„êµ¬ ì‚¬ìš© ê·œì¹™]
1. ì œê³µëœ ëŒ€í™”ì—ì„œ ëª¨ë“  ì£¼ë¬¸ ê´€ë ¨ ì •ë³´ë¥¼ ë©´ë°€íˆ ë¶„ì„í•©ë‹ˆë‹¤.
2. 'extract_order_info' ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ëœ ì •ë³´ë¥¼ ì§€ì •ëœ JSON ìŠ¤í‚¤ë§ˆì— ë§ì¶° êµ¬ì„±í•©ë‹ˆë‹¤.
3. ì‘ë‹µì€ ë°˜ë“œì‹œ ë„êµ¬ë¥¼ í†µí•´ ê·¸ ë„êµ¬ì˜ 'input_schema'ì— ì •ì˜ëœ JSON êµ¬ì¡°ë¡œ ë°˜í™˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: <tool_use name="extract_order_info">JSON_ë°ì´í„°</tool_use> ì™€ ìœ ì‚¬í•œ ë‚´ë¶€ì  ë„êµ¬ í˜¸ì¶œ ê²°ê³¼)
4. JSON ë°ì´í„°ëŠ” ë„êµ¬ì˜ input_schemaë¥¼ ì—„ê²©íˆ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤: {json.dumps(tools_for_fallback[0]["input_schema"], ensure_ascii=False, indent=2)}
5. ì–´ë–¤ ìƒí™©ì—ì„œë„ ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ, ì„¤ëª…, ì£¼ì„ ë“±ì„ í¬í•¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

ì£¼ë¬¸ ì •ë³´ ì¶”ì¶œ ì‹œ ë‹¤ìŒ ì‚¬í•­ì— ìœ ì˜í•˜ì„¸ìš”:
- íŒë§¤ìê°€ ì˜¬ë¦° ê³µì§€ì™€ ê³ ê° ì£¼ë¬¸ êµ¬ë¶„
- ë™ì¼ì¸, ë™ì¼ í’ˆëª© ì¤‘ë³µ ì£¼ë¬¸ ì‹œ ìµœì‹ ìœ¼ë¡œ ë°˜ì˜
- í’ˆëª©ëª…, ì£¼ë¬¸ìëª…, ìˆ˜ëŸ‰ ì •í™•íˆ ì¶”ì¶œ
- ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë¦¬ëœ ì£¼ë¬¸ ëª©ë¡, í’ˆëª©ë³„ ìš”ì•½, ì£¼ë¬¸ìë³„ ìš”ì•½ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
5. ëˆ„ë½ëœ ì£¼ë¬¸ ë‚´ì—­ì´ ì—†ë„ë¡ í‘œì‹œí•˜ê³ , ë°ì´í„°ëŠ” ìƒëµí•˜ê±°ë‚˜ "**ì™¸ nëª…, **ë“± nëª…ì²˜ëŸ¼" ì•½ì‹ìœ¼ë¡œ ê¸°ì¬í•˜ì§€ ì•Šê³  ì „ì²´ ê°’ì„ ëª¨ë‘ ì œê³µí•©ë‹ˆë‹¤.
6. ëŒ€í™”ì—ì„œ "ë§ˆê°"ìœ¼ë¡œ í‘œì‹œëœ ìƒí’ˆì€ íŒë§¤ê°€ ì¢…ë£Œëœ ìƒí’ˆì…ë‹ˆë‹¤.
7. ì£¼ë¬¸ íŒ¨í„´ ë¶„ì„ì—ì„œëŠ” ì‹œê°„ëŒ€ë³„ ì£¼ë¬¸ ê±´ìˆ˜, ì¸ê¸° ìƒí’ˆ, í’ˆì ˆ ìƒí’ˆì„ ì¶”ì¶œí•˜ì„¸ìš”.

ìµœëŒ€í•œ ë§ì€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ë˜, ì–´ë–¤ ê²½ìš°ì—ë„ 'extract_order_info' ë„êµ¬ë¥¼ í†µí•´ì„œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
"""
    if shop_name:
        fallback_system_prompt += f"\në¶„ì„ ì¤‘ì¸ ëŒ€í™”ëŠ” '{shop_name}' ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤."

    text_to_process_for_fallback = original_user_prompt_for_fallback

    if len(text_to_process_for_fallback) > 15000:
        logging.info(f"ëŒ€ì²´ ì²˜ë¦¬: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ 15,000ìë¥¼ ì´ˆê³¼ ({len(text_to_process_for_fallback)}ì). ë¶„í•  ë° ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘.")
        print(f"ëŒ€ì²´ ì²˜ë¦¬: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ 15,000ìë¥¼ ì´ˆê³¼ ({len(text_to_process_for_fallback)}ì). ë¶„í•  ë° ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘.")
        
        text_chunks_for_fallback = _split_input_text(text_to_process_for_fallback, max_length=15000)
        logging.info(f"ëŒ€ì²´ ì²˜ë¦¬: í…ìŠ¤íŠ¸ê°€ {len(text_chunks_for_fallback)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨.")
        
        all_results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(text_chunks_for_fallback), 3)) as executor:
            future_to_chunk_index = {
                executor.submit(
                    _process_fallback_chunk, 
                    chunk_text, i, len(text_chunks_for_fallback), 
                    shop_name, fallback_system_prompt, tools_for_fallback
                ): i for i, chunk_text in enumerate(text_chunks_for_fallback)
            }
            for future in concurrent.futures.as_completed(future_to_chunk_index):
                chunk_idx = future_to_chunk_index[future]
                try:
                    result = future.result()
                    if result: all_results.append(result)
                except Exception as exc:
                    logging.error(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_idx} ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {exc}\n{traceback.format_exc()}")

        if all_results:
            logging.info(f"ëŒ€ì²´ ì²˜ë¦¬: {len(all_results)}ê°œ ì²­í¬ ê²°ê³¼ ë³‘í•© ì¤‘")
            merged_result = _merge_chunk_results(all_results)
            log_file_path = _save_api_response_to_file(json.dumps(merged_result, ensure_ascii=False), f"{shop_name}_fallback_merged")
            logging.info(f"ëŒ€ì²´ ì²˜ë¦¬: ë³‘í•©ëœ ê²°ê³¼ê°€ {log_file_path} íŒŒì¼ì— ì €ì¥ë¨.")
            return _validate_and_process_result(merged_result, conversation_chunk)
        else:
            logging.warning("ëŒ€ì²´ ì²˜ë¦¬ (ë³‘ë ¬): ëª¨ë“  ë¶„í•  ì²­í¬ì—ì„œ ìœ íš¨í•œ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í•¨. ê¸°ë³¸ JSON êµ¬ì¡° ìƒì„±.")
            return _create_default_result(available_products, shop_name)
    else:
        logging.info(f"ëŒ€ì²´ ì²˜ë¦¬: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ì§§ìŒ ({len(text_to_process_for_fallback)}ì). ë‹¨ì¼ ëŒ€ì²´ í˜¸ì¶œ ì‹œë„.")
        print(f"ëŒ€ì²´ ì²˜ë¦¬: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ì§§ìŒ ({len(text_to_process_for_fallback)}ì). ë‹¨ì¼ ëŒ€ì²´ í˜¸ì¶œ ì‹œë„.")
        single_fallback_result = _process_fallback_chunk(
            chunk_text=text_to_process_for_fallback, chunk_index=0, total_chunks=1,
            shop_name=shop_name, system_prompt_for_fallback_chunk=fallback_system_prompt,
            tools_for_fallback_chunk=tools_for_fallback
        )
        if single_fallback_result:
            log_file_path = _save_api_response_to_file(json.dumps(single_fallback_result, ensure_ascii=False), f"{shop_name}_fallback_single")
            logging.info(f"ëŒ€ì²´ ì²˜ë¦¬(ë‹¨ì¼): ê²°ê³¼ê°€ {log_file_path} íŒŒì¼ì— ì €ì¥ë¨.")
            return _validate_and_process_result(single_fallback_result, conversation_chunk)
        else:
            logging.warning("ëŒ€ì²´ ì²˜ë¦¬ (ë‹¨ì¼): ìœ íš¨í•œ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í•¨. ê¸°ë³¸ JSON êµ¬ì¡° ìƒì„±.")
            print("ëŒ€ì²´ ì²˜ë¦¬ (ë‹¨ì¼): ìœ íš¨í•œ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í•¨. ê¸°ë³¸ JSON êµ¬ì¡° ìƒì„±.")
            return _create_default_result(available_products, shop_name)
    
def _process_fallback_chunk(
    chunk_text: str, 
    chunk_index: int, 
    total_chunks: int, 
    shop_name: Optional[str], 
    system_prompt_for_fallback_chunk: str, 
    tools_for_fallback_chunk: List[Dict]
) -> Optional[Dict[str, Any]]:
    """
    ëŒ€ì²´ ì²˜ë¦¬ ì‹œ ê°œë³„ ì²­í¬ë¥¼ LLMìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. (ë³‘ë ¬ ì‹¤í–‰ìš©, tool ì‚¬ìš© ê°•ì œ)
    """
    logging.info(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}/{total_chunks} ë¶„ì„ ì‹œì‘ (ê¸¸ì´: {len(chunk_text)}ì)")
    print(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}/{total_chunks} ë¶„ì„ ì‹œì‘ (ê¸¸ì´: {len(chunk_text)}ì)")
    
    result_json_obj = None
    full_text_content_stream = "" 
    tool_input_json_parts = [] 
    tool_actually_used = False

    try:
        # í‘œì¤€ client.messages.create ì‚¬ìš©, thinking ì œê±°, tool_choice ê°•ì œ, temperature 0.1
        stream_response = client.messages.create(
            model="claude-3-7-sonnet-20250219",
            max_tokens=4096, # ë„êµ¬ ì‚¬ìš© ì‹œì—ëŠ” ê²°ê³¼ JSON í¬ê¸°ì— ë§ì¶° ì ì ˆíˆ ì¡°ì ˆ
            system=system_prompt_for_fallback_chunk,
            temperature=0.1,
            tools=tools_for_fallback_chunk,
            tool_choice={"type": "tool", "name": "extract_order_info"},
            stream=True,
            messages=[
                {"role": "user", "content": chunk_text}
            ]
        )

        # Removed fallback_stream_chunks file-based logging

        if tool_actually_used and tool_input_json_parts:
            complete_tool_input_json_str = "".join(tool_input_json_parts)
            logging.info(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}: í•©ì³ì§„ ë„êµ¬ ì…ë ¥ JSON (ê¸¸ì´ {len(complete_tool_input_json_str)}): {complete_tool_input_json_str[:300]}...")
            
            # Removed fallback_raw_json file-based logging

            try:
                result_json_obj = json.loads(complete_tool_input_json_str)
                logging.info(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}: ë„êµ¬ ì…ë ¥ JSON íŒŒì‹± ì„±ê³µ.")
            except json.JSONDecodeError as e_json:
                logging.error(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}: ë„êµ¬ ì…ë ¥ JSON íŒŒì‹± ì˜¤ë¥˜ ({e_json}). ì›ë³¸: {complete_tool_input_json_str[:500]}")
                try:
                    fixed_json = _fix_json_string(complete_tool_input_json_str)
                    result_json_obj = json.loads(fixed_json)
                    logging.info(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}: ìˆ˜ì •ëœ ë„êµ¬ JSON íŒŒì‹± ì„±ê³µ.")
                except Exception as e_fix:
                    logging.error(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}: ìˆ˜ì •ëœ ë„êµ¬ JSON íŒŒì‹±ë„ ì‹¤íŒ¨ ({e_fix}).")
        
        if result_json_obj is None and full_text_content_stream:
            logging.info(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}: ë„êµ¬ ê²°ê³¼ ì—†ê³  í…ìŠ¤íŠ¸ ì‘ë‹µ ìˆìŒ, JSON ì¶”ì¶œ ì‹œë„.")
            result_json_obj = _extract_json_from_text(full_text_content_stream)
            if result_json_obj:
                 logging.info(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}: í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ì„±ê³µ.")
            else:
                 logging.warning(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}: í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ì‹¤íŒ¨.")

        if result_json_obj:
            logging.info(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}: ìœ íš¨í•œ JSON ê²°ê³¼ ì¶”ì¶œ ì„±ê³µ.")
            _save_api_response_to_file(json.dumps(result_json_obj, ensure_ascii=False), f"{shop_name}_fallback_chunk_{chunk_index+1}")
            return result_json_obj
        else:
            logging.warning(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1}: ìµœì¢…ì ìœ¼ë¡œ ê²°ê³¼ ì¶”ì¶œ ì‹¤íŒ¨.")
            return None

    except anthropic.APIError as e_api:
        logging.error(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1} Anthropic API ì˜¤ë¥˜: {str(e_api)}\n{traceback.format_exc()}")
        return None
    except Exception as e_gen:
        logging.error(f"ëŒ€ì²´ ì²˜ë¦¬ ì²­í¬ {chunk_index+1} ë¶„ì„ ì¤‘ ì¼ë°˜ ì˜¤ë¥˜ ë°œìƒ: {str(e_gen)}\n{traceback.format_exc()}")
        return None

def _create_default_result(available_products: Optional[Set[str]] = None, shop_name: Optional[str] = None) -> Dict[str, Any]:
    """
    ê¸°ë³¸ ê²°ê³¼ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        available_products (Set[str], optional): ì¶”ì¶œëœ ìƒí’ˆ ëª©ë¡
        shop_name (str, optional): ìƒì  ì´ë¦„
        
    Returns:
        dict: ê¸°ë³¸ ê²°ê³¼ êµ¬ì¡°
    """
    logging.info("ìµœì†Œ JSON êµ¬ì¡° ìƒì„± ì¤‘")
    print("ìµœì†Œ JSON êµ¬ì¡° ìƒì„± ì¤‘")
    
    result = {
        "time_based_orders": [],
        "item_based_summary": [],
        "customer_based_orders": [],
        "table_summary": {
            "headers": ["í’ˆëª©", "ì´ìˆ˜ëŸ‰", "ì£¼ë¬¸ì"],
            "rows": []
        },
        "order_pattern_analysis": {
            "peak_hours": [],
            "popular_items": [],
            "sold_out_items": []
        }
    }
    
    if shop_name:
        result["shop_name"] = shop_name
    
    # ìƒí’ˆ ëª©ë¡ì´ ìˆìœ¼ë©´ ê¸°ë³¸ item_based_summary ìƒì„±
    if available_products:
        for product in available_products:
            result["item_based_summary"].append({
                "item": product,
                "total_quantity": 0,
                "customers": ""
            })
    
    return result

def _merge_chunk_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ì—¬ëŸ¬ ì²­í¬ì—ì„œ ì–»ì€ ê²°ê³¼ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.
    
    Args:
        results (list): ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        dict: ë³‘í•©ëœ ê²°ê³¼
    """
    if not results:
        return {}
    
    # ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
    merged = results[0].copy()
    
    # ë°°ì—´ í˜•íƒœ í•„ë“œ ë³‘í•©
    array_fields = ["time_based_orders", "item_based_summary", "customer_based_orders"]
    for field in array_fields:
        if field not in merged:
            merged[field] = []
            
        # ë‚˜ë¨¸ì§€ ê²°ê³¼ì—ì„œ í•´ë‹¹ í•„ë“œ ë³‘í•©
        for result in results[1:]:
            if field in result and isinstance(result[field], list):
                merged[field].extend(result[field])
    
    # item_based_summary ì¤‘ë³µ ì œê±° ë° í†µí•©
    if "item_based_summary" in merged:
        item_dict = {}
        for item in merged["item_based_summary"]:
            if "item" not in item or not item["item"]:
                continue
                
            item_name = item["item"]
            if item_name not in item_dict:
                item_dict[item_name] = item.copy()
            else:
                # ìˆ˜ëŸ‰ í•©ì‚°
                try:
                    current_qty = item_dict[item_name].get("total_quantity", 0)
                    new_qty = item.get("total_quantity", 0)
                    
                    # ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜
                    if isinstance(current_qty, str):
                        current_qty = int(current_qty.replace(",", ""))
                    if isinstance(new_qty, str):
                        new_qty = int(new_qty.replace(",", ""))
                        
                    item_dict[item_name]["total_quantity"] = current_qty + new_qty
                except (ValueError, TypeError):
                    # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë˜ ê°’ ìœ ì§€
                    pass
                
                # ê³ ê° ëª©ë¡ ë³‘í•©
                current_customers = item_dict[item_name].get("customers", "")
                new_customers = item.get("customers", "")
                
                if current_customers and new_customers:
                    current_set = set(c.strip() for c in current_customers.split(","))
                    new_set = set(c.strip() for c in new_customers.split(","))
                    merged_set = current_set.union(new_set)
                    item_dict[item_name]["customers"] = ", ".join(sorted(list(merged_set)))
                elif new_customers:
                    item_dict[item_name]["customers"] = new_customers
        
        merged["item_based_summary"] = list(item_dict.values())
    
    # order_pattern_analysis ë³‘í•©
    if "order_pattern_analysis" in merged:
        for result in results[1:]:
            if "order_pattern_analysis" not in result:
                continue
                
            for field in ["peak_hours", "popular_items", "sold_out_items"]:
                if field in result["order_pattern_analysis"] and isinstance(result["order_pattern_analysis"][field], list):
                    if field not in merged["order_pattern_analysis"]:
                        merged["order_pattern_analysis"][field] = []
                    
                    merged["order_pattern_analysis"][field].extend(result["order_pattern_analysis"][field])
        
        # ì¤‘ë³µ ì œê±°
        for field in ["peak_hours", "popular_items", "sold_out_items"]:
            if field in merged["order_pattern_analysis"]:
                merged["order_pattern_analysis"][field] = list(set(merged["order_pattern_analysis"][field]))
    
    logging.info(f"ì²­í¬ ê²°ê³¼ ë³‘í•© ì™„ë£Œ: time_based_orders={len(merged.get('time_based_orders', []))}, "
                f"item_based_summary={len(merged.get('item_based_summary', []))}, "
                f"customer_based_orders={len(merged.get('customer_based_orders', []))}")
    
    return merged

def _validate_and_process_result(result: Dict[str, Any], preprocessed_text: str) -> Dict[str, Any]:
    """
    LLM ê²°ê³¼ë¥¼ ê²€ì¦í•˜ê³ , ì½”ë“œ ê¸°ë°˜ìœ¼ë¡œ item/table ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•˜ì—¬ ìµœì¢… ê²°ê³¼ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    
    Args:
        result (dict): LLM ë¶„ì„ ê²°ê³¼ (time_based_orders, customer_based_orders, order_pattern_analysis í¬í•¨ ê¸°ëŒ€)
        preprocessed_text (str): ì „ì²˜ë¦¬ëœ ëŒ€í™” ë‚´ìš© (í˜„ì¬ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        
    Returns:
        dict: ê²€ì¦ ë° ëª¨ë“  ì •ë³´ê°€ í¬í•¨ëœ ìµœì¢… ê²°ê³¼
    """
    # LLM ê²°ê³¼ì—ì„œ í•„ìš”í•œ ê¸°ë³¸ í•„ë“œ í™•ì¸ ë° ì´ˆê¸°í™”
    if not isinstance(result, dict):
        print("âš ï¸ ê²½ê³ : LLM ê²°ê³¼ê°€ ìœ íš¨í•œ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")
        result = {}

    llm_time_orders = result.get("time_based_orders", [])
    if not isinstance(llm_time_orders, list):
        print("âš ï¸ ê²½ê³ : LLM ê²°ê³¼ì—ì„œ 'time_based_orders'ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        llm_time_orders = []

    llm_customer_orders = result.get("customer_based_orders", [])
    if not isinstance(llm_customer_orders, list):
         print("âš ï¸ ê²½ê³ : LLM ê²°ê³¼ì—ì„œ 'customer_based_orders'ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
         llm_customer_orders = []

    llm_pattern_analysis = result.get("order_pattern_analysis", {})
    if not isinstance(llm_pattern_analysis, dict):
         print("âš ï¸ ê²½ê³ : LLM ê²°ê³¼ì—ì„œ 'order_pattern_analysis'ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
         llm_pattern_analysis = {"peak_hours": [], "popular_items": [], "sold_out_items": []}

    # LLMì´ ìƒì„±í•œ time_based_orders/customer_based_orders ê²€ì¦ (í•„ìš” ì‹œ)
    # ì˜ˆ: llm_time_orders = filter_invalid_items(llm_time_orders)
    #     llm_customer_orders = filter_invalid_items(llm_customer_orders)
    print(f"LLM ì¶”ì¶œ time_based_orders: {len(llm_time_orders)}ê°œ ì£¼ë¬¸")
    print(f"LLM ì¶”ì¶œ customer_based_orders: {len(llm_customer_orders)}ê°œ ì£¼ë¬¸ (ê³ ê°ë³„ ìƒì„¸)")

    # time_based_ordersë¥¼ ê¸°ë°˜ìœ¼ë¡œ item/table ìš”ì•½ ì •ë³´ ìƒì„±
    generated_summaries = _generate_item_and_table_summaries(llm_time_orders)

    # ìµœì¢… ê²°ê³¼ ì¡°í•©
    final_result = {
        "time_based_orders": llm_time_orders,
        "customer_based_orders": llm_customer_orders, # LLM ê²°ê³¼ ì‚¬ìš©
        "item_based_summary": generated_summaries["item_based_summary"], # ì½”ë“œ ìƒì„± ê²°ê³¼ ì‚¬ìš©
        "table_summary": generated_summaries["table_summary"], # ì½”ë“œ ìƒì„± ê²°ê³¼ ì‚¬ìš©
        "order_pattern_analysis": llm_pattern_analysis
    }

    # ê²°ê³¼ ìš”ì•½ ë¡œê·¸ ì¶œë ¥
    print(f"ì½”ë“œ ìƒì„± item_based_summary: {len(final_result.get('item_based_summary', []))}ê°œ í’ˆëª©")
    print(f"ì½”ë“œ ìƒì„± table_summary: {len(final_result.get('table_summary', {}).get('rows', []))}ê°œ í–‰")

    # ì£¼ë¬¸ íŒ¨í„´ ë¶„ì„ ê²€ì¦ (ê¸°ì¡´ ë¡œì§ í™œìš© ë˜ëŠ” ìˆ˜ì •)
    if "order_pattern_analysis" in final_result and isinstance(final_result["order_pattern_analysis"], dict):
        if "sold_out_items" in final_result["order_pattern_analysis"] and isinstance(final_result["order_pattern_analysis"]["sold_out_items"], list):
            original_count = len(final_result["order_pattern_analysis"]["sold_out_items"])
            final_result["order_pattern_analysis"]["sold_out_items"] = [
                item for item in final_result["order_pattern_analysis"]["sold_out_items"]
                if isinstance(item, str) and is_valid_item_name(item)
            ]
            filtered_count = original_count - len(final_result["order_pattern_analysis"]["sold_out_items"])
            if filtered_count > 0:
                print(f"sold_out_itemsì—ì„œ {filtered_count}ê°œì˜ ì˜ëª»ëœ í’ˆëª©ì´ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ë¹ˆ ë°°ì—´/ë°ì´í„° í™•ì¸ ë¡œê·¸ (ì„ íƒ ì‚¬í•­)
    # ... (ê¸°ì¡´ê³¼ ìœ ì‚¬í•˜ê²Œ í•„ìš”í•œ ê²€ì‚¬ ì¶”ê°€) ...
    
    return final_result

def _generate_item_and_table_summaries(time_based_orders: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    time_based_orders ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ item_based_summaryì™€ table_summaryë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    pandas ì—†ì´ êµ¬í˜„ë©ë‹ˆë‹¤.

    Args:
        time_based_orders: LLMì´ ì¶”ì¶œí•œ ì‹œê°„ìˆœ ì£¼ë¬¸ ëª©ë¡

    Returns:
        ìƒì„±ëœ item_based_summaryì™€ table_summaryë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
    """
    item_summary_data = defaultdict(lambda: {'total_quantity': 0, 'customers': set()})

    # ìˆ˜ëŸ‰ íƒ€ì… ë³€í™˜ í•¨ìˆ˜ (ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)
    def safe_int(value):
        if isinstance(value, int):
            return value
        if isinstance(value, str):
            try:
                return int(value.replace(',', ''))
            except ValueError:
                return 0
        return 0

    # 1. ë°ì´í„° ì§‘ê³„ (item_based_summary ì¤€ë¹„)
    for order in time_based_orders:
        item_name = order.get('item')
        customer_name = order.get('customer')
        quantity = safe_int(order.get('quantity', 0))

        if not item_name or not customer_name or quantity <= 0:
            continue

        # í’ˆëª©ë³„ ìš”ì•½ ë°ì´í„° ì—…ë°ì´íŠ¸
        item_summary_data[item_name]['total_quantity'] += quantity
        item_summary_data[item_name]['customers'].add(customer_name)

    # 2. item_based_summary ìƒì„±
    item_based_summary = []
    for item_name, data in item_summary_data.items():
        item_based_summary.append({
            'item': item_name,
            'total_quantity': data['total_quantity'],
            'customers': ', '.join(sorted(list(data['customers'])))
        })
    item_based_summary = sorted(item_based_summary, key=lambda x: x['total_quantity'], reverse=True)

    # 3. table_summary ìƒì„±
    table_summary_rows = []
    for item_data in item_based_summary: # item_based_summary ê²°ê³¼ë¥¼ ì‚¬ìš©
         table_summary_rows.append([
             item_data['item'],
             str(item_data['total_quantity']),
             item_data['customers']
         ])
    table_summary = {
        'headers': ["í’ˆëª©", "ì´ìˆ˜ëŸ‰", "ì£¼ë¬¸ì"],
        'rows': table_summary_rows
    }

    return {
        "item_based_summary": item_based_summary,
        "table_summary": table_summary,
    }
